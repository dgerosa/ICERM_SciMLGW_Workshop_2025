{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Bayesian analysis for GW population inference\n",
    "\n",
    "Our goal will be to self-consistency infer population parameters $\\Lambda$ by sampling this posterior:\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\Lambda|\\{d\\},\\{\\theta\\},) \\propto p(\\Lambda) e^{-N_{\\rm exp}(\\Lambda)}\\prod_i^{N_{\\rm ev}} p(d_i|\\theta_i) \\frac{{\\rm d} N}{{\\rm d} \\theta} (\\theta_i;\\Lambda)\n",
    "\\end{equation}\n",
    "\n",
    "We will go through each object in turn and then put it all together at the end, with a short demo on sampling with HMC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT THE BELOW IF RUNNING ON GOOGLE COLAB\n",
    "# !pip install numpy matplotlib h5ify\n",
    "# !pip install jax numpyro arviz\n",
    "# !pip install wcosmo\n",
    "# !mkdir -p inputs\n",
    "# !wget https://github.com/afarah18/HBA-for-GWs-tutorial/raw/refs/heads/main/inputs/pe.h5 -P inputs\n",
    "# !wget https://github.com/afarah18/HBA-for-GWs-tutorial/raw/refs/heads/main/inputs/endo3_bbhpop-LIGO-T2100113-v9.hdf5 -P inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5ify\n",
    "\n",
    "# inference (numpyro and friends)\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "import arviz as az\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "from jax import jit\n",
    "\n",
    "# our own packages\n",
    "import wcosmo\n",
    "wcosmo.disable_units()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\frac{{\\rm d} N (\\Lambda)}{{\\rm d} \\theta} $\n",
    "This is the merger rate density of CBCs: the expected number of events per single-event-parameter ($\\theta$) hypervolume, conditioned on hyper-parameters $\\Lambda$. Colloquially, this is called \"the population model.\" \n",
    "It is common to model this as \n",
    "\\begin{equation}\n",
    "    \\frac{{\\rm d} N}{{\\rm d} \\theta} (\\theta;\\Lambda) = \\mathcal{R}p(\\theta|\\Lambda) ,\n",
    "\\end{equation}\n",
    "where $\\mathcal{R}$ is the total merger rate and $p(\\theta|\\Lambda)$ is normalized over $\\theta$.\n",
    "\n",
    "We will adopt a simple parametric form for $p(\\theta|\\Lambda)$ using power laws, assume independent distributions of masses and redshifts, and ignore spins and any extrinsic parameters:\n",
    "\n",
    "\\begin{aligned}\n",
    "    p(m_1,q,z|\\Lambda) & = p(m_1,q|\\lambda_m)p(z,\\lambda_z)\n",
    "\\end{aligned}\n",
    "where  \n",
    "\\begin{aligned}\n",
    "    \\lambda_m &=\\{m_{\\min},m_{\\max},\\alpha,\\beta\\}, \\\\\n",
    "    \\lambda_z &=\\{z_{\\max},\\kappa\\}, \\\\\n",
    "    p(m_1,q|\\lambda_m) &\\propto \\begin{cases}\n",
    "    m_1^{\\alpha} &\\mathrm{ if }\\ m_1\\in [m_{\\min},m_{\\max}] \\\\\n",
    "    0 &\\mathrm{ otherwise} \\\\ \n",
    "    \\end{cases} \\times \\begin{cases}\n",
    "    q^{\\beta} &\\mathrm{ if } q \\in [m_{\\min}/m_1,1] \\\\\n",
    "    0 &\\mathrm{ otherwise} \\\\ \n",
    "    \\end{cases}\\ \\mathrm{ , and} \\\\\n",
    "    p(z|\\lambda_z) &\\propto \\frac{{\\rm d}V_c}{{\\rm d}z} \\frac{1}{1+z} \\begin{cases}\n",
    "    (1+z)^{\\kappa} &\\mathrm{ if }\\ z\\in [0,z_{\\max}] \\\\\n",
    "    0 &\\mathrm{ otherwise} \\\\ \n",
    "    \\end{cases} .\n",
    "\\end{aligned}\n",
    "Here, ${\\rm d}V_c/{\\rm d}z$ is the differential comoving volume. We need to choose a maximum redshift, $z_{\\max}$ so that our probability distributions are normalizable. This is not a physical parameter and is therefore always fixed, not fit along with the other hyperparameters. It should be chosen to lie far beyond the detector horizon so as not to have an impact on the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZMAX = 1.9\n",
    "\n",
    "@jit\n",
    "def _smoothed_powerlaw_unnormed(x,slope,minimum,maximum,smoothing):\n",
    "    low_start = minimum + smoothing\n",
    "    high_start = maximum - smoothing\n",
    "    low_filter = jnp.exp(-(x - low_start)**2/(2.*smoothing**2))\n",
    "    low_filter = jnp.where(x < low_start,low_filter,1.)\n",
    "    high_filter = jnp.exp(-(x - high_start)**2/(2.*smoothing**2))\n",
    "    high_filter = jnp.where(x > high_start,high_filter,1.)\n",
    "    return jnp.power(x,slope) * low_filter * high_filter\n",
    "\n",
    "@jit\n",
    "def _smoothed_powerlaw(x,slope,minimum,maximum,smoothing):\n",
    "    \"\"\"a power law distribution that is smoothed at the edges (for HMC happiness).\n",
    "    p(x|slope,minimum,maximum) \\propto x^slope between minimum and maximum. \n",
    "    Approximately normalized, (for small smoothing lengths).\n",
    "    \"\"\"\n",
    "    approxnorm = (1 + slope) / jnp.array(jnp.power(maximum, 1 + slope) - jnp.power(minimum, 1 + slope))\n",
    "    return _smoothed_powerlaw_unnormed(x,slope,minimum,maximum,smoothing) / approxnorm\n",
    "\n",
    "@jit\n",
    "def p_m1(m1,alpha,mmin,mmax):\n",
    "    \"\"\"a power law primary mass distribution. \"\"\"\n",
    "    return _smoothed_powerlaw(x=m1,slope=alpha,minimum=mmin,maximum=mmax,smoothing=.8)\n",
    "\n",
    "@jit\n",
    "def p_q_given_m1(q, m1, beta, mmin):\n",
    "    \"\"\"a power law mass ratio distribution. \n",
    "    Note that the minimum allowed mass ratio depends on m1, so this is always conditional on m1.\"\"\"\n",
    "    return _smoothed_powerlaw(x=q,slope=beta,minimum=mmin/m1,maximum=1.,smoothing=0.01)\n",
    "\n",
    "@jit \n",
    "def p_z_unnormed(z,kappa,zmax=ZMAX):\n",
    "    \"\"\"a power law redshift distribution, normalized s.t. the powerlaw returns 1 at z=0.\"\"\"\n",
    "    _zmin = 1e-3 # just set this to be close to zero to avoid zero-division errors\n",
    "    dVc_dz = wcosmo.Planck15.differential_comoving_volume(z) * 4 * jnp.pi \n",
    "    return dVc_dz * _smoothed_powerlaw_unnormed(1+z, kappa - 1, 1+_zmin, 1+zmax, smoothing=0.02)\n",
    "\n",
    "@jit\n",
    "def p_z(z,kappa,zmax=ZMAX):\n",
    "    \"\"\"a power law redshift distribution, normalized to a aribtrary maximum redshift zmax.\"\"\"\n",
    "    _zmin = 1e-3 # just set this to be close to zero to avoid zero-division errors\n",
    "    _z_dummy = jnp.linspace(_zmin,zmax,num=int(1e3))\n",
    "    norm = jnp.trapezoid(y=p_z_unnormed(_z_dummy,kappa=kappa,zmax=zmax), x=_z_dummy)\n",
    "    return p_z_unnormed(z,kappa,zmax) / norm\n",
    "\n",
    "@jit\n",
    "def p_theta_given_Lambda(m1,q,z,alpha,mmin,mmax,beta,kappa,zmax=ZMAX):\n",
    "    return p_m1(m1,alpha,mmin,mmax) * p_q_given_m1(q,m1,beta,mmin) * p_z(z,kappa,zmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m1_plotting = np.linspace(0,110,num=200)\n",
    "q_plotting = np.linspace(0,1,num=100)\n",
    "z_plotting = np.linspace(0,3,num=100)\n",
    "\n",
    "# TODO: fill me in!\n",
    "Lambda = dict(\n",
    "    alpha=\n",
    "    beta=\n",
    "    kappa=\n",
    "    mmin=\n",
    "    mmax=\n",
    ")\n",
    "\n",
    "plt.semilogy(m1_plotting,p_m1(m1_plotting,Lambda['alpha'],Lambda['mmin'],Lambda['mmax']))\n",
    "plt.xlabel(\"$m_1 [M_{\\odot}]$\")\n",
    "plt.ylabel(\"$p(m_1) [M_{\\odot}^{-1}]$\")\n",
    "plt.ylim(5e-8,1e-2)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(q_plotting,p_q_given_m1(q_plotting,m1=35,beta=Lambda['beta'],mmin=Lambda['mmin']))\n",
    "plt.xlabel(\"$q$\")\n",
    "plt.ylabel(\"$p(q|m_1)$\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(z_plotting,p_z(z_plotting,Lambda['kappa']))\n",
    "plt.xlabel(\"$z$\")\n",
    "plt.ylabel(\"$p(z)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $p(d_i|\\theta_i)$\n",
    "This is the single-event likelihood: the probability of observing the data $d_i$ associated with event $i$ given single-event paramters $\\theta_i$. Yesterday's tutorials and talks discussed how we can get single-event posteriors though parameter estimation (PE). Instead of repeating the whole PE process for every event in the catalog while we simultanously infer population parameters, we are going to use a trick called \"sample recycling.\" This uses posterior samples pre-calculated with yesterday's method (often called \"PE samples\") and divides by their prior (\"the PE prior\", $\\pi_{\\rm PE}$) to get samples from the single-event likelihoods. This is the most common method of sampling from $p(d_i|\\theta_i)$ in GW population studies.\n",
    "\n",
    "To do the sample recylcing trick, we first load in PE samples done for GWTC-3. To simplify things, this only includes events detected in O3. Note that the full PE results have lots of information in them that is not used in population analyses. Thus, a lot of data cleaning has been done on the samples we are about to load in (thanks, Matt!). Consider using `gwpopulation_pipe` to do similar data cleaning on samples that you download from GWTC data releases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in PE samples\n",
    "posteriors_T = h5ify.load('inputs/pe.h5')\n",
    "# re-arrange the data and ignore all parameters except for masses and redshift\n",
    "N_PE_samples = posteriors_T['GW190412']['mass_1_source'].size\n",
    "posteriors = {\n",
    "    k: jnp.array([posteriors_T[event][k] for event in sorted(posteriors_T)])\n",
    "    for k in [\"mass_1_source\",\"mass_ratio\",\"redshift\",\"prior\"]\n",
    "}\n",
    "# take a look at the first sample of the first 10 events\n",
    "{k:posteriors[k][:10,0] for k in posteriors.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note about event selection\n",
    "GW detection pipelines identify many GW event candidates. We are then free to set selection criteria on these candidates for inclusion in population analyses. However, it is crucial that these criteria are applied consistently throughout the analysis. Any choice made on real GW event candidates must be made on injections. These choices will set $N_{\\rm ev}$ and $N^{\\rm inj}_{\\rm found}$, so you can think of setting these criteria as equivalent to defining what a catalog is.\n",
    "\n",
    "For example, a choice based on real event parameters is impossible to apply consistently between injections and real events, as we don't know the true event parameters before doing a population analysis, but we do know the true parameters for injections. In practice, we often break this rule: a BBH-only analysis requires a cut on true secondary mass (usually $m_2\\gtrsim2.5 M_{\\odot}$) to exclude NS-containing events, but its important to know when you are breaking this rule and test that the effects are small. It turns out that cutting on mass is a smaller effect than using different significance thresholds (e.g. true SNR, observed SNR, FAR) for injections and data. \n",
    "\n",
    "**The events we loaded in here are BBHs ($m_2>3 M_{\\odot}$) detected in O3 with a false alarm rate (FAR) of less than $1/{\\rm year}$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $p(d_i|\\theta_i) \\frac{{\\rm d} N}{{\\rm d}\\theta}(\\theta_i;\\Lambda)$\n",
    "\n",
    "Putting the above two sections together, we can now get the expression in the product in the population likelihood, namely\n",
    "\n",
    "\\begin{equation}\n",
    "p(d_i|\\theta_i) \\frac{{\\rm d} N}{{\\rm d}\\theta}(\\theta_i;\\Lambda).\n",
    "\\end{equation}\n",
    "\n",
    "This is proportional to the population-informed individual event posterior for each event.\n",
    "\n",
    "Dividing this by the total merger rate yeilds $p(d_i|\\theta_i) p(\\theta|\\Lambda)$. Since $p(\\theta|\\Lambda)$ can be interpreted as a prior on $\\theta$ (usually called the \"population prior\"), the above expression is proportional to the \"population-informed individual-event posterior.\" We will examine this for a single event. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arbitrarily choose one event from the catalog\n",
    "example_event = posteriors_T['GW190728_064510']\n",
    "# divide by the PE prior to get the single-event likelihood, multiply by the population prior to get the population-informed posterior\n",
    "\n",
    "weights = p_theta_given_Lambda(m1=example_event['mass_1_source'],q=example_event['mass_ratio'],z=example_event['redshift'],**Lambda\n",
    "                               )/example_event['prior']\n",
    "\n",
    "plt.hist(example_event['mass_1_source'],weights=weights,bins=50,density=True,histtype='step',label=f\"$\\\\alpha=$ {Lambda['alpha']}\")\n",
    "\n",
    "# compare with another choice of primary mass power law slope\n",
    "Lambda2 = Lambda.copy()\n",
    "Lambda2['alpha']= # TODO: fill me in!\n",
    "weights2 = p_theta_given_Lambda(m1=example_event['mass_1_source'],q=example_event['mass_ratio'],z=example_event['redshift'],**Lambda2\n",
    "                               )/example_event['prior']\n",
    "plt.hist(example_event['mass_1_source'],weights=weights2,bins=50,density=True,histtype='step',label=f\"$\\\\alpha=${Lambda2['alpha']}\")\n",
    "\n",
    "plt.xlabel(\"$m_1 [M_{\\odot}]$\")\n",
    "plt.ylabel(\"posterior density $[M_{\\odot}^{-1}]$\")\n",
    "plt.legend()\n",
    "plt.xlim(right=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The population prior has a noticable effect on the parameters we infer for our events! This is why population analyses are necessary for individual-event interpretation.\n",
    "\n",
    "Often, we marginalize over the possible properties of each event:\n",
    "\\begin{equation}\n",
    "    p(\\{d\\}|\\Lambda) \\propto e^{-N_{\\text{exp}}(\\Lambda)}\\prod_i^{N_{\\text{ev}}} \\int \n",
    "    d\\theta_i\\, p(d_i|\\theta_i) \\frac{{\\rm d} N}{ {\\rm d} \\theta} (\\theta_i;\\Lambda)\n",
    "\\end{equation}\n",
    "If we wished to simultaneously infer the properties of individual events along with the population hyper-parameters, we simply would not marginalize over those events' parameters, and instead perform a high-dimensional inference, as in [ Mancarella & Gerosa 2025](https://ui.adsabs.harvard.edu/abs/2025PhRvD.111j3012M/abstract).\n",
    "\n",
    "We can do this marginalization with a Monte Carlo sum over PE samples:\n",
    "\\begin{equation}\n",
    "    \\int {\\rm d} \\theta_i\\, p(d_i|\\theta_i) \\frac{{\\rm d} N}{{\\rm d} \\theta} (\\theta_i;\\Lambda) \\approx \\frac{1}{N_{\\rm samps}} \\sum_{k=1}^{N_{\\rm samps}} \\frac{\\frac{{\\rm d} N}{{\\rm d} \\theta} (\\theta_{k,i};\\Lambda)}{\\pi_{\\rm PE}(\\theta_{k,i})} .\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the MC integral for the above-considered event is simply\n",
    "weights.sum() / N_PE_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $N_{\\rm exp}(\\Lambda)$\n",
    "\n",
    "This is the expected number of detections in our catalog, as defined by our event selection. This is simply the rate of CBCs in the Universe (${\\rm d} N/{\\rm d} \\theta$) multiplied by the probability of detecting an event with those parameters and given a noise realization $n$, $P(\\det|\\theta,n)$.\n",
    "The total number of expected detections is then obtained simply by summing the contributions from each value of every source parameter, i.e. marginalizing over $\\theta$ and $t_{{\\rm d}et}$:\n",
    "\\begin{equation}\n",
    "    N_{\\text{exp}}(\\Lambda) = \\int{\\rm d}\\theta \\frac{{\\rm d} N}{ {\\rm d} \\theta} (\\theta;\\Lambda) P(\\det|\\theta,n) \\, .\n",
    "\\end{equation}\n",
    "\n",
    "*Aside:* GW experiments are unique because $P(\\det|\\theta,n)$ is precisely knowable for GWs from CBCs, which is nontrivial for other astrophysical messengers, which typically have large theoretical uncertainties in the models that connect their source parameters to their observability.\n",
    "Because GR provides a complete mapping from source properties $\\theta$ to the GW signal, we are able to \"undo\" GW selection biases.\n",
    "Often in the GW literature, all possible noise realizations are marginalized over to write $P(\\det|\\theta)$.\n",
    "In practice, though, this marginalization is not precise as a complete detector noise model does not exist.\n",
    "Thus, analyses either work under the assumption of stationary Gaussian detector noise, or the observed strain data is used as the detector noise realization and no marginalization is performed.\n",
    "\n",
    "$N_{\\text{exp}}(\\Lambda)$ is typically calculated using \"software injections.\"\n",
    "Software injections are a large set of $N_{\\rm inj}$ samples drawn from a known distribution $p_{\\rm draw}(\\theta^{\\rm inj})$.\n",
    "These are then \"injected\" in the data stream by calculating the waveforms that would be made by each draw's source parameters and adding this to the strain measured by the detectors.\n",
    "Finally, the search pipelines that are used to identify real GW events are run on the data stream that has been injected with the simulated signals, resulting in significance estimates for each injection, such as a FAR.\n",
    "Applying *the same selection criteria as we defined for real events* to these injections results in $N_{\\rm found}$ found injections with known true parameters $\\theta^{\\rm inj}$.\n",
    "$N_{\\text{exp}}(\\Lambda)$ can then be approximated with a Monte-Carlo sum over *found* injections, as $P(\\det|\\theta^{\\rm inj},n)$ is zero when the injection is not found:\n",
    "\\begin{equation}\n",
    "    N_{\\text{exp}}(\\Lambda) \\approx \\frac{1}{N_{\\rm inj}}\\sum_j^{N_{\\rm found}} \\frac{{\\rm d} N}{{\\rm d} \\theta} (\\theta^{\\rm inj};\\Lambda) \\frac{1}{p_{\\rm draw}(\\theta^{\\rm inj}_j)} .\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load O3 software injections and only keep fields we will use. Some of the below code is borrowed from gwpopulation_pipe\n",
    "full_injections = h5ify.load('inputs/endo3_bbhpop-LIGO-T2100113-v9.hdf5')['injections']\n",
    "keys_subset = [\"mass1_source\",\"mass2_source\",\"redshift\",\"mass1_source_mass2_source_sampling_pdf\",\"redshift_sampling_pdf\",]\n",
    "for substr in [\"far\"]:\n",
    "    keys_subset += [\n",
    "        key\n",
    "        for key in full_injections.keys()\n",
    "        if any(\n",
    "            [\n",
    "                key.startswith(f\"{substr}_\"),\n",
    "                key.endswith(f\"_{substr}\"),\n",
    "                f\"_{substr}_\" in key,\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "injections = {k : full_injections[k] for k in keys_subset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select injections based on catalog definition \n",
    "threshold_FAR =  # TODO: fill this in with the false alarm rate used to select events above\n",
    "threshold_m2 =  # TODO: fill this in with the mass cut used to select BBHs above \n",
    "\n",
    "# First, select on what we consider to be a BBH \n",
    "# (again, this is technically inconsistent between real events and injections because the latter is on the data level and the former is on the event-parameter level).\n",
    "BBH_injections = {k : injections[k][ injections['mass2_source'] > threshold_m2 ] for k in keys_subset}\n",
    "N_inj = len(BBH_injections['mass1_source'])\n",
    "\n",
    "# Then, select on the significance of each trigger. \n",
    "# We typically consider an event that meets this criteria in *any* detection pipeline to be part of the catalog, \n",
    "# hence the \"or\" statement below (i.e. |= )\n",
    "found_bool = np.zeros(N_inj,dtype=bool)\n",
    "for key in keys_subset:\n",
    "    if \"far\" in key:\n",
    "        found_bool |= BBH_injections[key] < threshold_FAR\n",
    "# NOTE: We are downsampling the found injections to 10,000 for the sake of this tutorial just to make the inference run faster. \n",
    "# This is very bad, don't do this in production runs! It will increase your MC variance and that's a big issue for us.\n",
    "found_BBH_injections = {k : jnp.array(BBH_injections[k][found_bool][:10000],dtype='float64') for k in keys_subset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the terms in the Nexp sum\n",
    "# we will reuse the same Lambda as above, just for demonstration\n",
    "\n",
    "# first, dN/dtheta(theta;Lambda)\n",
    "local_rate_per_time = 15 / 1e9 # per kpc^3 per year\n",
    "observing_time = 1 # 1 year in O3\n",
    "z_dummy = jnp.linspace(0,ZMAX,num=int(1e3))\n",
    "total_rate = local_rate_per_time * observing_time * np.trapz(y=p_z_unnormed(z_dummy,kappa=Lambda['kappa'])) \n",
    "\n",
    "dN_dtheta = # TODO: fill me in!\n",
    "\n",
    "# second, pdraw\n",
    "pdraw = found_BBH_injections['mass1_source_mass2_source_sampling_pdf'] * found_BBH_injections['redshift_sampling_pdf']\n",
    "# this is defined over m1, m2, and z. However, p(theta|Lambda) is defind over m1, q, and z, \n",
    "# so we need to multiply by the Jacobian between m2 and q to match units\n",
    "pdraw *= found_BBH_injections['mass1_source'] \n",
    "found_BBH_injections['pdraw'] = pdraw\n",
    "\n",
    "# finally, put them together\n",
    "weights = dN_dtheta / pdraw\n",
    "Nexp = np.sum(weights) / N_inj\n",
    "# display the number of expected BBH in O3 under this population model. Is it consistent with how many events we observed in O3?\n",
    "Nexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: change Lambda and see how Nexp is affected. \n",
    "# Can you guess what hyperparameter choices would increase or decrease the expected number of observations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get a feel for the selction effects, plot P(det | theta) for each parameter. This is not sensitive to pdraw or Lambda\n",
    "plt.hist(found_BBH_injections['mass1_source'],density=True,bins=50,histtype='step',\n",
    "         weights=1/found_BBH_injections['pdraw']\n",
    ")\n",
    "plt.xlabel(\"$m_1 [M_{\\odot}]$\")\n",
    "plt.ylabel(\"observed population $[M_{\\odot}^{-1}]$\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(found_BBH_injections['mass2_source']/found_BBH_injections['mass1_source'],density=True,bins=50,histtype='step',\n",
    "         weights=1/found_BBH_injections['pdraw']\n",
    "         )\n",
    "plt.xlabel(\"mass ratio\")\n",
    "plt.ylabel(\"observed population $[M_{\\odot}^{-1}]$\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(found_BBH_injections['redshift'],density=True,bins=50,histtype='step',\n",
    "         weights=1/found_BBH_injections['pdraw']\n",
    "         )\n",
    "plt.xlabel(\"redshift\")\n",
    "plt.ylabel(\"observed population\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling the population posterior\n",
    "We are now ready to put this all together and perform a hierarchical Bayesian inference to infer $\\Lambda$. For this, we will use a probablistic programming library that allows us to more intuitively follow the DAG:\n",
    "\n",
    "<img src=\"DAG.png\" alt=\"DAG\" width=\"300\"/>\n",
    "\n",
    "I'm using `numpyro`, but you could use `pyro`, `stan`, or another probabilistic programming language in a similar fashion. In `numpyro`, \n",
    "```python\n",
    "x = numpyro.sample(\"name\",dist.p(q))\n",
    "``` \n",
    "samples a random variable, `x` named `\"name\"` from a probability distribution `dist.p` defined parametrized by `q`, and records `p(x|q)` this as the prior probability of `x`. It is equivalent to $x\\sim p(x|q)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the population posterior\n",
    "def population_posterior(data,found_injections, N_total_injections, observing_time):\n",
    "    # Top level of the DAG: draw Lambda from p(Lambda). \n",
    "    # We will use uniform priors on most of our hyperparameters for simplicity\n",
    "    alpha = numpyro.sample(\"alpha\", dist.Uniform(   )) # TODO: fill in some prior bounds for this power law slope that you think are reasonable\n",
    "    beta = #TODO: fill me in!\n",
    "    kappa = #TODO: fill me in!\n",
    "    mmin = numpyro.sample(\"mmin\", dist.Uniform(3,10)) # use 3 as the minimum because our injections don't exist below there\n",
    "    mmax = numpyro.sample(\"mmax\", dist.Uniform(30,100)) # use 100 as maximum because our injections don't exist above there\n",
    "    local_rate = numpyro.sample('local_rate',dist.TruncatedNormal(loc=15.,scale=30.,low=0.01)) # Volumetric rate\n",
    "    total_number = local_rate * observing_time * jnp.trapezoid(y=p_z_unnormed(z_dummy,kappa=kappa)) / 1e9\n",
    "\n",
    "    # Prod [ \\int( p(d_i|theta_i) dN/dtheta(theta_i;Lambda) ) ]\n",
    "    ## single-event posteriors\n",
    "    \n",
    "    p_theta_i_given_Lambda = #TODO: fill me in!\n",
    "\n",
    "    ## marginalize over theta_i and multiply by the rate\n",
    "    dN_dtheta_i = jnp.sum( p_theta_i_given_Lambda / data['prior'] ,axis=1) \n",
    "    ## record the data terms. We take the log because numpyro expects log probabilities, and sum all single-event terms\n",
    "    numpyro.factor(\"logp\",jnp.sum(jnp.log(total_number) + jnp.log(dN_dtheta_i) - jnp.log(N_PE_samples)))\n",
    "\n",
    "    # Nexp\n",
    "    ## dN/dtheta(theta;Lambda)\n",
    "    dN_dtheta = total_number * p_theta_given_Lambda(\n",
    "        m1=found_injections['mass1_source'],\n",
    "        q=found_injections['mass2_source']/found_injections['mass1_source'],\n",
    "        z=found_injections['redshift'],\n",
    "        alpha=alpha,\n",
    "        beta=beta,\n",
    "        mmin=mmin,\n",
    "        mmax=mmax,\n",
    "        kappa=kappa,\n",
    ")\n",
    "    ## pdraw\n",
    "    pdraw = found_injections['pdraw']\n",
    "\n",
    "    Nexp = # TODO: fill me in!\n",
    "    ## record log( exp( -Nexp ) )  term\n",
    "    numpyro.factor(\"Nexp\",-1 * Nexp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample it with numpyro's Hamiltonian monte carlo method (the No U-Turn Sampler, specifically)\n",
    "jax_rng = jax.random.PRNGKey(42)\n",
    "nuts_settings = dict(target_accept_prob=0.9, max_tree_depth=10,dense_mass=False)\n",
    "nuts_kernel = numpyro.infer.NUTS(population_posterior,**nuts_settings)\n",
    "mcmc = numpyro.infer.MCMC(nuts_kernel,num_warmup=500,num_samples=500,\n",
    "                            num_chains=1,progress_bar=True)   \n",
    "\n",
    "mcmc.run(jax_rng,data=posteriors,found_injections=found_BBH_injections,N_total_injections=N_inj,observing_time=1)\n",
    "\n",
    "hyperpe_samples = mcmc.get_samples() \n",
    "mcmc.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagnostic plots\n",
    "az.plot_trace(mcmc, compact=True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "az.plot_pair(mcmc, kind='kde',marginals=True,var_names=[\"~MC_variance\",\"~Neff\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the posterior predictive distributions for the astrophysical population.\n",
    "## mass distribuiton\n",
    "fig,ax = plt.subplots(figsize=(6,10), nrows=2)\n",
    "line_kwargs = dict(color='b',lw=0.5,alpha=0.5)\n",
    "\n",
    "random_inds = np.random.choice(np.arange(hyperpe_samples['alpha'].size),size=200)\n",
    "for i in random_inds:\n",
    "    r = hyperpe_samples['local_rate'][i] * p_m1(m1_plotting,alpha=hyperpe_samples['alpha'][i],mmin=hyperpe_samples['mmin'][i],mmax=hyperpe_samples['mmax'][i])\n",
    "    ax[0].plot(m1_plotting,r,**line_kwargs)\n",
    "    ax[1].plot(z_plotting,hyperpe_samples['local_rate'][i] * p_z(z_plotting,kappa=hyperpe_samples['kappa'][i]),**line_kwargs)\n",
    "    \n",
    "ax[0].set_yscale('log')\n",
    "# ax[0].set_ylim(5e-2,1e2)\n",
    "ax[0].set_xlabel(\"$m_1 [M_{\\odot}]$\")\n",
    "ax[0].set_ylabel(\"$d N/d m_1 d V [M_{\\odot}^{-1} Gpc^{-3} yr^{-1}]$\")\n",
    "\n",
    "ax[1].set_xlim(0,0.8)\n",
    "# ax[1].set_ylim(-1,150)\n",
    "ax[1].set_xlabel(\"$z$\")\n",
    "ax[1].set_ylabel(\"$dN/dz$\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icerm_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
