{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e950352",
   "metadata": {},
   "source": [
    "# Building and Optimising Neural Network Surrogates with $\\texttt{gwbonsai}$\n",
    "\n",
    "Author: Lucy M Thomas\n",
    "\n",
    "Email: lmthomas@caltech.edu\n",
    "\n",
    "Date: 3rd June 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d375bfc",
   "metadata": {},
   "source": [
    "This notebook was created for a tutorial called 'Public Code Packages to Visualise and Optimise Gravitational Wave Surrogate Models'. \n",
    "\n",
    "The tutorial is given as part of a workshop session on Scientific Machine Learning for Gravitational Wave Astronomy, ICERM, Brown University, on 3rd June 2025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0fc080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import useful packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5710d4",
   "metadata": {},
   "source": [
    "We consider a simple subspace of non-spinning binary black hole mergers ($\\vec{\\chi}_{1}=\\vec{\\chi}_{2}=0$). We restrict to mass ratios between $q=1$ and $q=2$.\n",
    "\n",
    "Let's build a fit for the time-domain amplitude of the $(2,2)$-mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f12f6c5",
   "metadata": {},
   "source": [
    "## Create and Align the Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934504cb",
   "metadata": {},
   "source": [
    "First, let's generate some training data for our fit. We'll use the $\\texttt{NRSur7dq4}$ model to generate this training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2326d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first check that the installation of gwsurrogate was successful, and then download the surrogate model.\n",
    "import gwsurrogate as gws\n",
    "sur = gws.LoadSurrogate('NRSur7dq4') # This download might take a minute or two, depending on your internet connection speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d793bf",
   "metadata": {},
   "source": [
    "Now we'll define functions to generate training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2777b733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a simplified interface to the NRSur7dq4 model\n",
    "def NRSur7dq4_22_nonspinning(q, dt=0.1):\n",
    "    \"\"\" Simplified inferface to NRSur7dq4 to get the (2,2) mode amplitude for nonspinning systems.\n",
    "\n",
    "      INPUT\n",
    "      =====\n",
    "      q  -- mass ratio\n",
    "      dt -- timestep size, Units of M\"\"\"\n",
    "\n",
    "    chiA  = [0.0, 0.0, 0.0]        # dimensionless spin of the heavier BH\n",
    "    chiB  = [0.0, 0.0, 0.0]        # dimensionless spin of the lighter BH\n",
    "    f_low = 0.0065               # initial frequency in units of 1/M\n",
    "    f_ref = f_low                  # reference frequecny (1/M) spins defined at\n",
    "\n",
    "    times, h, dyn = sur(q, chiA, chiB, dt=dt, f_low=f_low, f_ref=f_ref)\n",
    "\n",
    "    return times, np.abs(h[(2,2)])\n",
    "\n",
    "def training_set_generator(N,dt=0.1,verbose=False):\n",
    "    \"\"\"Generate N training samples from q in [1,2]\"\"\"\n",
    "    qs = np.linspace(1.0,2.0,N)\n",
    "    training_data = []\n",
    "    for q in qs:\n",
    "        t,h = NRSur7dq4_22_nonspinning(q,dt=dt)\n",
    "        training_data.append(h)\n",
    "        if verbose:\n",
    "            print('length of h is %i'%len(h))\n",
    "    return qs, training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6656751",
   "metadata": {},
   "source": [
    "We'll start with 11 points between $q=1$ and $q=2$, and 11 points between $\\chi_{1z}\\in[0.,0.8]$, and start by visualising the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af038a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_samples = 11\n",
    "\n",
    "q_train, train_data = training_set_generator(num_train_samples, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd74b4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_train_samples):\n",
    "    times = 0.1 * np.arange(len(train_data[i]))\n",
    "    plt.plot(times,train_data[i], label='q=%.2f'%q_train[i])\n",
    "plt.xlabel('time (M)')\n",
    "plt.ylabel('$|h_{22}|$ (M)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdb19c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_time_grid(training_data,dt=0.1):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "    =====\n",
    "    training_data: set of training waveforms \n",
    "    \n",
    "    OUTPUT\n",
    "    ======\n",
    "    training data as a numpy array, padding with zeros as \n",
    "    necessary such that all waveforms are of the same length\"\"\"\n",
    "    \n",
    "    longest_waveform = 0\n",
    "    for h in training_data:\n",
    "        length = len(h)\n",
    "        if length > longest_waveform:\n",
    "            longest_waveform = length\n",
    "            \n",
    "    print(\"longest waveform size = %i\"%longest_waveform)\n",
    "        \n",
    "    padded_training_data = []\n",
    "    for h in training_data:\n",
    "        nZeros = longest_waveform - len(h)\n",
    "        h_pad = np.append(h, np.zeros(nZeros))\n",
    "        padded_training_data.append(h_pad)\n",
    "        \n",
    "    times = np.arange(longest_waveform)*dt\n",
    "    \n",
    "    padded_training_data = np.vstack(padded_training_data).transpose()\n",
    "    \n",
    "    return times, padded_training_data\n",
    "\n",
    "times, train_data = common_time_grid(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275fda69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_peak(t, h):\n",
    "  \"\"\"Get argument and values of t and h at maximum value of |h| on a discrete grid. \"\"\"\n",
    "  arg = np.argmax(np.abs(h))\n",
    "  return [arg, t[arg], h[arg]]\n",
    "\n",
    "def get_peaks(t,training_set):\n",
    "    \"\"\" Find the index of each waveform's peak in the entire training set. \"\"\"\n",
    "    time_peak_arg = []\n",
    "    for i in range(num_train_samples):\n",
    "        [arg, t_peak, h_peak] = get_peak(times,training_set[:,i]) # i^th training sample\n",
    "        time_peak_arg.append(arg)\n",
    "        print(\"Waveform %i with t_peak = %f\"%(i,t_peak))\n",
    "    print(time_peak_arg)\n",
    "    return time_peak_arg\n",
    "\n",
    "def align_peaks(times, training_set):\n",
    "    \"\"\" Peak align a set of waveforms. The shortest waveform is used as the reference\n",
    "    one.\"\"\"\n",
    "    \n",
    "    time_peak_arg = get_peaks(times,training_set)\n",
    "    \n",
    "    min_arg = min(time_peak_arg)\n",
    "    aligned_training_set=[]\n",
    "    for i in range(num_train_samples):\n",
    "        offset = time_peak_arg[i] - min_arg\n",
    "        print(\"offset value of %i\"%offset)\n",
    "        h_aligned = training_set[offset:,i]\n",
    "        aligned_training_set.append(h_aligned)\n",
    "        \n",
    "    t, training_data_aligned = common_time_grid(aligned_training_set)\n",
    "    return training_data_aligned\n",
    "\n",
    "train_data_aligned = align_peaks(times, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e202c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_train_samples):\n",
    "    plt.plot(times, train_data_aligned[:,i], label='q=%.2f'%q_train[i])\n",
    "plt.xlabel('time (M)')\n",
    "plt.ylabel('$|h_{22}|$ (M)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b956cdec",
   "metadata": {},
   "source": [
    "For the actual model, we'll probably need more than 11 training points, so let's generate 401. This might take a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d35ce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_samples = 401\n",
    "q, data = training_set_generator(num_train_samples, verbose=False)\n",
    "times, data = common_time_grid(data)\n",
    "data_aligned = align_peaks(times, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa8e49c",
   "metadata": {},
   "source": [
    "We'll find it useful to split this data set into a training, validation and test set for when we come to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990f80a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_train, q_validation, train_data, validation_data = sklearn.model_selection.train_test_split(q, data_aligned.T, random_state=0,test_size=0.4)\n",
    "q_test, q_validation, test_data, validation_data = sklearn.model_selection.train_test_split(q_validation, validation_data, random_state=0,test_size=0.5)\n",
    "print('Number of training samples: %i'%len(q_train))\n",
    "print('Number of validation samples: %i'%len(q_validation))\n",
    "print('Number of test samples: %i'%len(q_test))\n",
    "q_train = q_train.reshape(q_train.shape[0], 1)\n",
    "q_validation = q_validation.reshape(q_validation.shape[0], 1)\n",
    "q_test = q_test.reshape(q_test.shape[0], 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c08035",
   "metadata": {},
   "source": [
    "## Create the Bases and Empirical Interpolant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8cfd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rompy as rp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aecd4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "integration = rp.Integration([times[0], times[-1]], num=len(times), rule='trapezoidal')\n",
    "rb = rp.ReducedBasis(integration)\n",
    "rb.make(train_data, 0, 1e-10, verbose=True)\n",
    "# We could try reducing/increasing this tolerance to see how it affects the reduced basis size.\n",
    "print(\"Reduced basis dimension: %i\"%rb.size)\n",
    "eim = rp.EmpiricalInterpolant(rb.basis, verbose=True)\n",
    "print(\"Empirical interpolant completed.\")\n",
    "print(\"relative compression ratio: %.2f\"%(float((num_train_samples*len(times))/(rb.size*eim.size))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc860d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eim.make_data(validation_data)\n",
    "val_eim_data = eim.data\n",
    "eim.make_data(test_data)\n",
    "test_eim_data = eim.data\n",
    "eim.make_data(train_data)\n",
    "train_eim_data = eim.data\n",
    "train_eim_data = train_eim_data.reshape(train_eim_data.shape[1], train_eim_data.shape[0])\n",
    "val_eim_data = val_eim_data.reshape(val_eim_data.shape[1], val_eim_data.shape[0])\n",
    "test_eim_data = test_eim_data.reshape(test_eim_data.shape[1], test_eim_data.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c878b9d3",
   "metadata": {},
   "source": [
    "## Create A Simple Neural Network Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb169b0",
   "metadata": {},
   "source": [
    "We'll make a simple, sequential, fully connected multi-layer perceptron (MLP) to model the parametric fits of the EIM nodes across the parameter space of $q\\in[1,2]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f141de46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/lucythomas/ResearchProjects/gwbonsai/')\n",
    "import gwbonsai as gb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import InputLayer, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(123456)\n",
    "from contextlib import redirect_stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a7dab8",
   "metadata": {},
   "source": [
    "We'll compile a simple MLP to do the fit, and train it to see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b4b4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_prefix = '/Users/lucythomas/ResearchProjects/ICERM/' # Where to save the model\n",
    "input_shape = 1 # We have only one input, the mass ratio q\n",
    "output_shape = 9 # The number of EIM nodes\n",
    "num_hidden_layers = 4 # Number of hidden layers in the neural network\n",
    "nodes_per_layer = 10 # Number of nodes per hidden layer\n",
    "activation = 'relu' # Activation function for the hidden layers\n",
    "learning_rate = 1e-3 # Learning rate for the optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b75d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(shape=(input_shape,)))\n",
    "\n",
    "for layer in range(num_hidden_layers):\n",
    "    model.add(Dense(nodes_per_layer, activation=activation))\n",
    "    \n",
    "model.add(Dense(output_shape, activation='linear'))\n",
    "\n",
    "model.compile(\n",
    "    # Optimization algorithm, specify learning rate\n",
    "    optimizer=Adam(learning_rate=learning_rate),\n",
    "    # Loss function for a binary classifier\n",
    "    loss='mean_squared_error',\n",
    "    # Diagnostic quantities\n",
    "    #metrics=['mean_squared_error']\n",
    "    )\n",
    "# Saving summary of compiled model to model_summary.txt.\n",
    "with open(file_prefix+'model_summary.txt', 'w') as f:\n",
    "    with redirect_stdout(f):\n",
    "        model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560834a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(q_train, train_eim_data,epochs=100, batch_size=16, validation_data=(q_validation, val_eim_data), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098ab87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6635caf",
   "metadata": {},
   "source": [
    "Training history looks okay, a small amount of fitting after ~10 epochs but not too bad. Let's evaluate the surrogate model on the test set and plot the results. We'll plot a random one of the test waveform amplitudes to get an idea of how we're doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe1b98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = model.predict(q_test)\n",
    "predicted_test_data = np.dot(nodes, eim.B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d350f3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(times, predicted_test_data[26,:], label='q=%.2f (predicted)'%q_test[26])\n",
    "plt.plot(times, test_data[26,:], label='q=%.2f (true)'%q_test[26])\n",
    "plt.xlabel('time (M)')\n",
    "plt.ylabel('$|h_{22}|$ (M)')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf2ceb7",
   "metadata": {},
   "source": [
    "Gross, looks like our model seriously needs some work! But how do we achieve a better fit? \n",
    "\n",
    "Is it with more layers? More training data? A different activaton function? A bit of everything?\n",
    "\n",
    "How do we know (without lots of prior experience of this problem) which hyperparameters to change and which to leave?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2252652",
   "metadata": {},
   "source": [
    "## Motivation for Systematic Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aa09b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "img = Image.open('mentimeter_screenshot.png')\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e86ae8",
   "metadata": {},
   "source": [
    "Let's look back at the metimeter from the end of Melissa's session yesterday.\n",
    "\n",
    ">'tuning ml is painful'...\n",
    "\n",
    ">'ml is trial and error'...\n",
    "\n",
    ">'architecture is important'...\n",
    "\n",
    "What if there was a way we could try and simplify all this? To optimise the network and its training data in a more systematic way, to try and make sure we're getting the best surrogate we possibly can with a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85c795e",
   "metadata": {},
   "source": [
    "Enter: `gwbonsai` (Building and Optimising Neural Surrogates for Astrophysical Inference). \n",
    "\n",
    "It's a helper package that provides routines for systematically optimising the training of neural network gravitatioanl wave surrogate models. It's available at this Git repo [link](https://github.com/lucymthomas/gwbonsai/tree/main). \n",
    "\n",
    "The package is stll being actively developed, so if you're a person who would benefit from this and want to see features that don't exist yet, please let me know! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb9785b",
   "metadata": {},
   "source": [
    "It leverages the power of `Optuna` for optimising the hyperparameters of the model, and then optimises the amount and distribution of training data required to ahcieve a good fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce33b242",
   "metadata": {},
   "source": [
    "Practically, we found that for most surrogate-building applications, optimising all the architecture (not to mention the training dataset) all at once was prohibively expensive. Therefore we split the problem up into sections:\n",
    "    \n",
    "1. Optimise functional hyperparameter (network non-linear behaviour)\n",
    "2. Optimise network size and shape parameters (avoids overfitting)\n",
    "3. Optimise training dataset size and distribtuion (ensures good coverage of the parameter space)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdd37da",
   "metadata": {},
   "source": [
    "In the remainder of this tutorial, we will go over these three steps in details for our (2,2)-mode amplitude model, and hopefully achieve a more convincing surrogate at the end!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c44b0e7",
   "metadata": {},
   "source": [
    "## A Quick Aside: How Does the Hyperparameter Optimisation Work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2a25d2",
   "metadata": {},
   "source": [
    "The default hyperparameter optimisation used in `Optuna` is called a `TPE sampler`, which stands for `Tree-structured Parzen Estimator`.\n",
    "The mathematical idea behind the TPE sampler is based on Bayesian optimisation, and simultaneously models the distribution of 'good' points in hyperspace, and 'bad' ones.\n",
    "\n",
    "\n",
    "Let $x$ be the value of a hyperparameter\n",
    "\n",
    "Let $y$ be the value of the loss function that hyperparameter achieves\n",
    "\n",
    "We are trying to find the value of $x$ across the space that minimises $y$\n",
    "\n",
    "TPE key idea:\n",
    "\n",
    "model $P(x|y)$ rather than $P(y|x)$\n",
    "\n",
    "Throughout optimisation, we model two distributions:\n",
    "\n",
    "$P(x | y > y*) = P(x | bad)$\n",
    "\n",
    "$P(x | y =< y*) = P(x | good)$\n",
    "\n",
    "New samples are proposed by maximising the expected improvement:\n",
    "\n",
    "$EI = P(x | y =< y*) / P(x | y > y*)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e093230",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('TPE1.png')\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782eae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('TPE2.png')\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ecb70f",
   "metadata": {},
   "source": [
    "## Optimising Functional Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401dc4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "functional_options_dict = {\n",
    "    'activation': ['relu', 'tanh', 'sigmoid'],\n",
    "    'weight_init': ['glorot_uniform'], #['glorot_uniform', 'HeUniform', 'HeNormal'],\n",
    "    'learning_rate': [1e-4, 1e-3, 1e-2],\n",
    "    'optimiser': ['Adam','Adamax','Nadam','Ftrl', 'SGD', 'RMSprop'],\n",
    "    'normalisation': [0] #[0, 1], # Whether to use batch normalization\n",
    "}\n",
    "\n",
    "fixed_dict = {\n",
    "    'nodes_per_layer': 10,\n",
    "    'num_hidden_layers': 4,\n",
    "    'dropout': 1.0, # 1 for dropout layer, 0 for no dropout\n",
    "    'dropout_rate': 0.2, # dropout rate\n",
    "    'batch_size':16, #[8, 16, 32]\n",
    "    'num_epochs': 100,\n",
    "}\n",
    "input_dim = 1\n",
    "output_dim = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea1145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.samplers import TPESampler\n",
    "import optuna\n",
    "from functools import partial\n",
    "sampler = TPESampler(seed=123456)\n",
    "from gwbonsai.optimise_hyper.optimise_functional_tensorflow import functional_objective\n",
    "\n",
    "objective_partial = partial(functional_objective, input_dim=input_dim, output_dim=output_dim, functional_options_dict=functional_options_dict, fixed_dict=fixed_dict, x_train=q_train, train_eim_data=train_eim_data, x_validation=q_validation, val_eim_data=val_eim_data, x_test=q_test, test_eim_data=test_eim_data, eim=eim)\n",
    "study = optuna.create_study(direction='minimize',sampler=sampler)\n",
    "study.optimize(objective_partial, n_trials=50)\n",
    "\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "        \n",
    "best_params = trial.params\n",
    "np.save('best_functional_hyper.npy', best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4e4853",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.visualization import plot_contour\n",
    "from optuna.visualization import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0416488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contour(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886823ff",
   "metadata": {},
   "source": [
    "There are lots of additional great visualisation tools available in Optuna if you want to dive deeper into , for more information see their [documentation page](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/005_visualization.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325375a0",
   "metadata": {},
   "source": [
    "## Optimising Size and Shape Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2a2b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best functional hyperparameters obtained from functional_optimisation_7d.py\n",
    "functional_best_params = np.load('best_functional_hyper.npy', allow_pickle=True)\n",
    "functional_best_params\n",
    "fixed_dict = functional_best_params.item()\n",
    "fixed_dict['num_epochs'] = 100\n",
    "\n",
    "shape_options_dict = {\n",
    "    'num_hidden_layers': [2,4,6,8,10], # Number of hidden layers in the neural network\n",
    "    'nodes_per_layer': [8,10,15,20,50], # Number of nodes per hidden layer\n",
    "    'dropout': [0.0,1.0], # 1 for dropout layer, 0 for no dropout\n",
    "    'dropout_rate': np.linspace(0.1, 0.5, 5).tolist(), # dropout rate\n",
    "    'batch_size': [8] # Batch size for training\n",
    "}\n",
    "\n",
    "input_shape = 1\n",
    "output_shape = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9accb324",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = TPESampler(seed=123456)\n",
    "from gwbonsai.optimise_hyper.optimise_size_shape_tensorflow import shape_objective\n",
    "\n",
    "objective_partial = partial(shape_objective, input_dim=input_dim, output_dim=output_dim, shape_options_dict=shape_options_dict, fixed_dict=fixed_dict, x_train=q_train, train_eim_data=train_eim_data, x_validation=q_validation, val_eim_data=val_eim_data, x_test=q_test, test_eim_data=test_eim_data, eim=eim)\n",
    "study = optuna.create_study(direction='minimize',sampler=sampler)\n",
    "study.optimize(objective_partial, n_trials=100)\n",
    "\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "        \n",
    "best_params = trial.params\n",
    "np.save('best_shape_hyper.npy', best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9254573e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.visualization import plot_contour\n",
    "from optuna.visualization import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735ea3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contour(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc46fcd4",
   "metadata": {},
   "source": [
    "## Optimising Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5983a7",
   "metadata": {},
   "source": [
    "New we've obtained our optimal hyperparameters for the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5ee92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_functional = np.load('best_functional_hyper.npy', allow_pickle=True)\n",
    "best_shape = np.load('best_shape_hyper.npy', allow_pickle=True)\n",
    "best = best_functional | best_shape\n",
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8259d1a6",
   "metadata": {},
   "source": [
    "We will split the original training set into a smaller training set, and the rest will be a holdout set. The smaller training set will iteratively grow larger, as the worst performing points from the holdout set are added to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf045efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_holdout, q_train_0, holdout_eim_data, train_eim_data_0 = sklearn.model_selection.train_test_split(q_train, train_eim_data, random_state=0,test_size=0.1)\n",
    "print('Number of training samples: %i'%len(q_train_0))\n",
    "print('Number of holdout samples: %i'%len(q_holdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb09b5f",
   "metadata": {},
   "source": [
    "We create a list with the sizes of training datasets we wish to have over the course of the procedure- these list values must add up to (less than) the length of q_train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9cc944",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 10\n",
    "iteration_size = len(q_train) // num_iterations\n",
    "append_sizes = [iteration_size] * num_iterations\n",
    "\n",
    "assert np.sum(append_sizes) == 240, f\"Sum is {np.sum(append_sizes)}, expected 240\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc043004",
   "metadata": {},
   "source": [
    "Sert up dataframes to store our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e90c655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.DataFrame(train_eim_data, columns=['eim_0', 'eim_1', 'eim_2', 'eim_3', 'eim_4', 'eim_5', 'eim_6', 'eim_7', 'eim_8'])\n",
    "df_train['q']=q_train.flatten()\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314870b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(test_eim_data, columns=['eim_0', 'eim_1', 'eim_2', 'eim_3', 'eim_4', 'eim_5', 'eim_6', 'eim_7', 'eim_8'])\n",
    "df_test['q']=q_test.flatten()\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65297cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = ['q']\n",
    "output_cols = ['eim_0', 'eim_1', 'eim_2', 'eim_3', 'eim_4', 'eim_5', 'eim_6', 'eim_7', 'eim_8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fd3552",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_iteration = np.full(len(q_train), np.nan)\n",
    "first_iteration[:append_sizes[0]] = 0\n",
    "df_train['first_training_iteration'] = first_iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf44743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gwbonsai.optimise_data.optimise_data import train_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a79b98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_iteration(append_sizes, best, df_train, df_test, input_cols, output_cols, 100, eim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8aed84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ac409c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data1 = df_test['mean_error_0']\n",
    "data2 = df_test['mean_error_1']\n",
    "data3 = df_test['mean_error_2']\n",
    "data4 = df_test['mean_error_3']\n",
    "data5 = df_test['mean_error_4']\n",
    "data6 = df_test['mean_error_5']\n",
    "data7 = df_test['mean_error_6']\n",
    "data8 = df_test['mean_error_7']\n",
    "data9 = df_test['mean_error_8']\n",
    "data10 = df_test['mean_error_9']\n",
    "\n",
    "data = [data1, data2, data3, data4, data5, data6, data7, data8, data9, data10]\n",
    "positions = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19] # Custom positions for each violin\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Create violin plots\n",
    "ax.violinplot(data, positions=positions, showmeans=True)\n",
    "\n",
    "# Set the x-axis ticks and labels\n",
    "ax.set_xticks(positions)\n",
    "ax.set_xticklabels(['Mean error 1', 'Mean error 2', 'Mean error 3', 'Mean error 4', 'Mean error 5', 'Mean error 6', 'Mean error 7', 'Mean error 8', 'Mean error 9', 'Mean error 10'])\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Training Data Sets')\n",
    "ax.set_ylabel('Mean Squared Error')\n",
    "\n",
    "ax.set_yscale('log')\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f01b131",
   "metadata": {},
   "source": [
    "Where was the training data distributed? Where were extra points added?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896e798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data1 = df_test[df_train['mean_error_0'].notna()]['q']\n",
    "data2 = df_test[df_train['mean_error_1'].notna()]['q']\n",
    "data3 = df_test[df_train['mean_error_2'].notna()]['q']\n",
    "data4 = df_test[df_train['mean_error_3'].notna()]['q']\n",
    "data5 = df_test[df_train['mean_error_4'].notna()]['q']\n",
    "data6 = df_test[df_train['mean_error_5'].notna()]['q']\n",
    "data7 = df_test[df_train['mean_error_6'].notna()]['q']\n",
    "data8 = df_test[df_train['mean_error_7'].notna()]['q']\n",
    "data9 = df_test[df_train['mean_error_8'].notna()]['q']\n",
    "data10 = df_test[df_train['mean_error_9'].notna()]['q']\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Create violin plots\n",
    "ax.hist(data1, label='Iteration 0', histtype='step', bins=20, density=True)\n",
    "ax.hist(data2, label='Iteration 1', histtype='step', bins=20, density=True)\n",
    "ax.hist(data3, label='Iteration 2', histtype='step', bins=20, density=True)\n",
    "ax.hist(data4, label='Iteration 3', histtype='step', bins=20, density=True)\n",
    "ax.hist(data5, label='Iteration 4', histtype='step', bins=20, density=True)\n",
    "ax.hist(data6, label='Iteration 5', histtype='step', bins=20, density=True)\n",
    "ax.hist(data7, label='Iteration 6', histtype='step', bins=20, density=True)\n",
    "ax.hist(data8, label='Iteration 7', histtype='step', bins=20, density=True)\n",
    "ax.hist(data9, label='Iteration 8', histtype='step', bins=20, density=True)\n",
    "ax.hist(data10, label='Iteration 9', histtype='step', bins=20, density=True)\n",
    "\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('q')\n",
    "ax.set_ylabel('Density')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516cd15a",
   "metadata": {},
   "source": [
    "## Questions/Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c566410",
   "metadata": {},
   "source": [
    "How does our final fit compare to the original one at the beginning? Test it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267a5a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c4bc09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55823b90",
   "metadata": {},
   "source": [
    "Things we did today:\n",
    "\n",
    "    - Optimised the functional hyperparameters of the neural network\n",
    "    - Optimised the shape hyperparameters of the neural network\n",
    "    - Optimised the size and distribution of the training data\n",
    "Things we did not do:\n",
    "\n",
    "    - Changing the size of the reduced basis as our iterative training data set grows\n",
    "    - Optimising the size of the reduced basis for the accuracy we want to achieve\n",
    "    - Use more complicated neural netwrk architectures or features (learning rate schedulers, other kinds of architectures, hourglass architectures, etc.)\n",
    "    - Play around with data scaling (normalisation, standardisation, etc.)\n",
    "\n",
    "What else could we have done to improve our fits?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9860edbf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gwbonsai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
